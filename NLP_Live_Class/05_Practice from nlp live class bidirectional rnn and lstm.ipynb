{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os # if we want to change the path then we can use os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('file_I cannot upload here because it is more than 94 mb and i cannot commit in github')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classification using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Nan values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the independent features \n",
    "# here label column is the values like 1 and 0 which is dependent and other than that is indpendent features\n",
    "X = df.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dependent features\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the shape of dependent and independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "\n",
    "# Embedding : when computer doesn't know text, in nlp we convert the text into a kind of number and number to vector and vector is \n",
    "# a kind of representation of different words in sentences and for this we need Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "# we need preprocessing because we need the lenght of sequence as same \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = X.copy()\n",
    "\n",
    "# checking how the text look like \n",
    "\n",
    "messages['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index \n",
    "messages.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(0, len(messages)):\n",
    "    sent = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
    "    sent = sent.lower()\n",
    "    sent = sent.split()\n",
    "\n",
    "    sent = [stemmer.stem(word) for word in sent if not word in stopwords.words('english')]\n",
    "    sent = ' '.join(sent)\n",
    "    corpus.append(sent)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "voc_size = 1000000   # vocabulary size\n",
    "onehot_repe = [one_hot(words, voc_size) for words in corpus]\n",
    "onehot_repe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_doc = pad_sequences(onehot_repe, maxlen = 100, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "embedding_vector_features = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_vector_features, input_length = 100))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, actication = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrices = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.array(embedded_doc)\n",
    "y_input = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size= 0.2, random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and training the data \n",
    "model.fit(X_train, y_train, epochs = 5, batch_size = 64, validation_data = X_test, y_test)\n",
    "\n",
    "# Epochs : suppose we have 50K data and this 50K data will go forward and backward pass for 5 times \n",
    "# the model will train the total data for 5 times \n",
    "\n",
    "# validation data is basically used as a try run your model and check it's performance before giving real world test data. \n",
    "# while validation data we can fine tune our model using hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction / checking the performance \n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)> 0.5).astype(\"int64\")\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> 2\u001b[0m confusion_matrix(\u001b[43my_test\u001b[49m, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi- Directional LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "\n",
    "embedding_vector_features = 50\n",
    "\n",
    "model_bidirectional_lstm = Sequential()\n",
    "model_bidirectional_lstm.add(Embedding(voc_size, embedding_vector_features, input_length = 100))\n",
    "model_bidirectional_lstm.add(Bidirectional(LSTM(32))\n",
    "model_bidirectional_lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_bidirectional_lstm.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrices = ['accuracy'] )\n",
    "model_bidirectional_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional_lstm.fit(X_train, y_train, epochs = 5, batch_size = 64, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = (model_bidirectional_lstm.predict(X_test)> 0.5).astype(\"int64\")\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rnn Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "\n",
    "embedding_vector_features = 50\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(voc_size, embedding_vector_features, input_length = 100))\n",
    "model_rnn.add(SimpleRNN(32))\n",
    "model_rnn.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_rnn.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrices = ['accuracy'] )\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.fit(X_train, y_train, epochs = 5, batch_size = 64, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = (model_rnn.predict(X_test)> 0.5).astype(\"int64\")\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, Bidirectional\n",
    "\n",
    "\n",
    "embedding_vector_features = 50\n",
    "\n",
    "model_bidirectional_rnn = Sequential()\n",
    "model_bidirectional_rnn.add(Embedding(voc_size, embedding_vector_features, input_length = 100))\n",
    "model_bidirectional_rnn.add(Bidirectional(SimpleRNN(32)))\n",
    "model_bidirectional_rnn.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_bidirectional_rnn.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrices = ['accuracy'] )\n",
    "model_bidirectional_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional_rnn.fit(X_train, y_train, epochs = 5, batch_size = 64, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = (model_bidirectional_rnn.predict(X_test)> 0.5).astype(\"int64\")\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing different confusion matrix\n",
    "LSTM\n",
    "array([[3119,  300],\n",
    "       [ 219, 2397]])\n",
    "\n",
    "bi-LSTM\n",
    "array([[3115,  304],\n",
    "       [ 231, 2385]])\n",
    "\n",
    "RNN\n",
    "array([[2999,  420],\n",
    "       [  84, 2532]])\n",
    "\n",
    "\n",
    "bi-RNN\n",
    "array([[3129,  290],\n",
    "       [ 238, 2378]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout :   In case of large architecture, it will have high number of hidden layer so this leads to overfitting so to \n",
    "# get rid of overfitting we add dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "\n",
    "\n",
    "# Creating Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "\n",
    "\n",
    "embedding_vector_features = 50\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(voc_size, embedding_vector_features, input_length = 100))\n",
    "model4.add(Dropout(0.3)) # here 30 % of the hidden layer will drop out \n",
    "model4.add(LSTM(100))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model4.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrices = ['accuracy'] )\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit(X_train, y_train, epochs = 5, batch_size = 64, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = (model4.predict(X_test)> 0.5).astype(\"int64\")\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
