{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the reuqired library\n",
    "\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc2Vec is used for creating document embeddings. It captures the context of entire documents.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample text for practical\n",
    "\n",
    "text = \"Doc2Vec is used for creating document embeddings. It captures the context of entire documents.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize into sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing into sentences and words\n",
      "Tokenized sentences: [['doc2vec', 'is', 'used', 'for', 'creating', 'document', 'embeddings', '.'], ['it', 'captures', 'the', 'context', 'of', 'entire', 'documents', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing into sentences and words\")\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "print(\"Tokenized sentences:\", tokenized_sentences)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tagged documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing tagged documents\")\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Doc2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the Doc2Vec model\n",
      "Doc2Vec model trained successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train the Doc2Vec model\")\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, dm=1, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"Doc2Vec model trained successfully\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer document vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring document vectors\n",
      "Inferred document vector: [-3.7892056e-03 -9.1003720e-04  1.3252553e-03  4.1811527e-03\n",
      " -3.4329444e-03 -1.4843595e-03  4.1820551e-03  4.1065766e-03\n",
      " -2.0369673e-03  2.2983882e-03  9.9527114e-04  1.8706385e-03\n",
      "  1.4475896e-03  9.2713657e-04 -3.7892105e-03  1.7110766e-03\n",
      " -4.9955975e-03  3.8456519e-03 -1.4362973e-03  3.5737930e-03\n",
      " -2.4352912e-03 -3.9127977e-03 -1.5075003e-04 -4.8330831e-03\n",
      "  1.6774440e-03  3.3472131e-03 -3.9221407e-03  4.9427943e-03\n",
      " -4.5712994e-04  1.0239341e-04  1.3449680e-03  2.1689739e-03\n",
      "  1.5758956e-03  1.0761607e-03  3.3921301e-03  2.3175841e-03\n",
      "  2.7489138e-03  2.1536578e-03  4.5455829e-03 -3.2869186e-03\n",
      "  4.4557618e-04  4.2993193e-03 -1.0418779e-03 -2.2210407e-03\n",
      " -8.7684119e-04  9.0524042e-04  5.0871979e-05 -1.6323179e-03\n",
      "  1.4909770e-03 -9.2365628e-04 -1.7471384e-03 -4.5598536e-03\n",
      " -2.5568535e-03 -3.7487512e-03  2.4220655e-03  1.5855645e-03\n",
      " -4.0235491e-03  3.3668266e-03  4.0582358e-03  3.6161209e-03\n",
      " -2.0283465e-03  2.6760649e-04 -1.8113529e-03  1.3041741e-03\n",
      "  3.1518885e-03 -4.2312690e-03 -6.9349137e-04  4.0542013e-03\n",
      "  4.6267901e-03 -3.1835097e-03 -7.3043437e-04  3.4141082e-03\n",
      "  3.7193084e-03 -2.5392603e-03  4.0539862e-03 -1.4575971e-03\n",
      "  3.1066944e-03 -4.5000180e-03 -1.7568662e-03 -1.7221113e-04\n",
      " -3.3324633e-03  2.8070866e-03  7.7611301e-04  1.0649061e-03\n",
      "  2.6709135e-03 -2.7218868e-03 -3.1088714e-03  4.7977665e-03\n",
      " -4.1357689e-03  1.6936599e-03  4.6350802e-03 -3.9066435e-03\n",
      "  2.0210077e-03 -3.5492848e-03  4.0912698e-03 -2.5699092e-03\n",
      "  2.0002734e-03 -2.1967273e-03 -4.4462522e-03 -4.7685280e-03]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferring document vectors\")\n",
    "doc_vector = model.infer_vector(word_tokenize(\"Doc2Vec is a powerful tool for document embeddings.\"))\n",
    "print(\"Inferred document vector:\", doc_vector)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support@pwskills.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
