{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the reuqired library\n",
    "\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc2Vec is used for creating document embeddings. It captures the context of entire documents.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample text for practical\n",
    "\n",
    "text = \"Doc2Vec is used for creating document embeddings. It captures the context of entire documents.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize into sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing into sentences and words\n",
      "Tokenized sentences: [['doc2vec', 'is', 'used', 'for', 'creating', 'document', 'embeddings', '.'], ['it', 'captures', 'the', 'context', 'of', 'entire', 'documents', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing into sentences and words\")\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "print(\"Tokenized sentences:\", tokenized_sentences)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tagged documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing tagged documents\")\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Doc2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the Doc2Vec model\n",
      "Doc2Vec model trained successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train the Doc2Vec model\")\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, dm=1, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"Doc2Vec model trained successfully\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer document vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering document vectors\n",
      "Inferred document vector: [-3.8413841e-03 -9.6587243e-04  1.2822049e-03  4.2428784e-03\n",
      " -3.4659868e-03 -1.5062141e-03  4.1416907e-03  4.1624922e-03\n",
      " -2.0395233e-03  2.2970436e-03  1.0200735e-03  1.8484684e-03\n",
      "  1.4011435e-03  8.8950986e-04 -3.7751931e-03  1.6184825e-03\n",
      " -5.0244536e-03  3.8770179e-03 -1.4584443e-03  3.5635170e-03\n",
      " -2.4003519e-03 -3.8530671e-03 -1.8980850e-04 -4.7652214e-03\n",
      "  1.6642825e-03  3.3244414e-03 -4.0195473e-03  4.9300776e-03\n",
      " -4.3796326e-04  2.9727047e-05  1.3628080e-03  2.2126269e-03\n",
      "  1.5748548e-03  1.0884719e-03  3.3459174e-03  2.3015023e-03\n",
      "  2.7488552e-03  2.1034258e-03  4.4801598e-03 -3.3130862e-03\n",
      "  4.5728724e-04  4.3028551e-03 -1.0275168e-03 -2.3165785e-03\n",
      " -8.9369150e-04  8.7271270e-04  6.5214255e-05 -1.5871155e-03\n",
      "  1.5119698e-03 -9.4704283e-04 -1.7782578e-03 -4.5106341e-03\n",
      " -2.5577673e-03 -3.8290219e-03  2.4180464e-03  1.6273867e-03\n",
      " -4.0323180e-03  3.4187252e-03  4.0243650e-03  3.6692361e-03\n",
      " -2.0018849e-03  3.2814121e-04 -1.7298078e-03  1.2617763e-03\n",
      "  3.1505320e-03 -4.2802361e-03 -6.9409836e-04  4.0347571e-03\n",
      "  4.5483871e-03 -3.2205884e-03 -7.2192249e-04  3.4418034e-03\n",
      "  3.6963043e-03 -2.5824625e-03  4.0667001e-03 -1.4371920e-03\n",
      "  3.1471814e-03 -4.5788563e-03 -1.7930329e-03 -1.8371122e-04\n",
      " -3.2818487e-03  2.8730321e-03  7.0220220e-04  1.0955394e-03\n",
      "  2.6406595e-03 -2.6650811e-03 -3.0845099e-03  4.7619832e-03\n",
      " -4.0710508e-03  1.7332596e-03  4.5675728e-03 -3.8431538e-03\n",
      "  2.0206000e-03 -3.5469031e-03  4.0830001e-03 -2.5008835e-03\n",
      "  2.0360195e-03 -2.2267222e-03 -4.3720254e-03 -4.7060479e-03]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Infering document vectors\")\n",
    "doc_vector = model.infer_vector(word_tokenize(\"Doc2Vec is a powerful tool for document embeddings.\"))\n",
    "print(\"Inferred document vector:\", doc_vector)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
